import os
import re
import json
import yaml
import requests
from datetime import datetime
from jinja2 import Environment, FileSystemLoader
import markdown

# ==================== è·¯å¾„é…ç½® ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(BASE_DIR, "config.yml")
TEMPLATE_DIR = os.path.join(BASE_DIR, "template")
DOCS_DIR = os.path.join(BASE_DIR, "docs") # docs ç›®å½•
ARTICLE_DIR = os.path.join(DOCS_DIR, "article")
OMD_DIR = os.path.join(BASE_DIR, "O-MD")
OMD_JSON = os.path.join(OMD_DIR, "articles.json")
BASE_YAML_OUT = os.path.join(BASE_DIR, "base.yaml")

# æ–°å¢ï¼šæœ¬åœ° Posts ç›®å½• (ä½äº docs ç›®å½•ä¸‹)
LOCAL_POSTS_DIR = os.path.join(DOCS_DIR, "posts")

DEFAULT_ARTICLE_TEMPLATE = "article.html"
DEFAULT_HOME_TEMPLATE = "home.html"

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(ARTICLE_DIR, exist_ok=True)
os.makedirs(OMD_DIR, exist_ok=True)
# åˆ›å»ºæœ¬åœ° posts ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(LOCAL_POSTS_DIR, exist_ok=True)


class VaLogGenerator:
    def __init__(self):
        print("=" * 50)
        print("ğŸš€ VaLog Generator åˆå§‹åŒ–ä¸­...")

        # åŠ è½½é…ç½®
        self.config = {}
        if os.path.exists(CONFIG_PATH):
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f) or {}
        
        # è¯»å–æ•°æ®æºæ¨¡å¼
        self.data_source_mode = self.config.get('data_source_mode', 'dual').lower()
        print(f"ğŸ“‹ æ•°æ®æºæ¨¡å¼: {self.data_source_mode}")

        self.article_template_name = self.config.get('templates', {}).get(
            'VaLog-default-article', DEFAULT_ARTICLE_TEMPLATE
        )
        self.home_template_name = self.config.get('templates', {}).get(
            'VaLog-default-index', DEFAULT_HOME_TEMPLATE
        )

        # åŠ è½½å¹¶è¿ç§»ç¼“å­˜
        self.cache = self._load_and_migrate_cache()

        # Jinja2 æ¨¡æ¿å¼•æ“
        self.env = Environment(
            loader=FileSystemLoader(TEMPLATE_DIR),
            autoescape=False,
            trim_blocks=True,
            lstrip_blocks=True
        )
        # --- ä¼˜åŒ–ç‚¹ 2: é¢„åŠ è½½æ¨¡æ¿ ---
        self.article_template = self.env.get_template(self.article_template_name)
        self.home_template = self.env.get_template(self.home_template_name)

    def _load_and_migrate_cache(self):
        """åŠ è½½ç¼“å­˜å¹¶å¤„ç†æ—§æ ¼å¼åˆ°æ–°æ ¼å¼çš„è¿ç§»"""
        cache = {}
        if os.path.exists(OMD_JSON):
            try:
                with open(OMD_JSON, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        # æ£€æŸ¥å¹¶è¿ç§»æ—§æ ¼å¼ç¼“å­˜
        # æ—§æ ¼å¼: { "issue_number": "updated_at_string" }
        # æ–°æ ¼å¼: { "id": { "type": "issue|local_file", "last_modified": "..." } }
        migrated = False
        for key, value in list(cache.items()): # ä½¿ç”¨ list() é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            # å¦‚æœå€¼æ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯æ—§æ ¼å¼ (ä¸€å®šæ˜¯ issue)
            if isinstance(value, str):
                print(f"ğŸ”„ è¿ç§»æ—§ç¼“å­˜æ¡ç›®: #{key}")
                cache[key] = {
                    "type": "issue",
                    "last_modified": value
                }
                migrated = True
        
        if migrated:
            print("ğŸ’¾ ä¿å­˜è¿ç§»åçš„ç¼“å­˜...")
            with open(OMD_JSON, 'w', encoding='utf-8') as f:
                json.dump(cache, f, indent=2, ensure_ascii=False)
        
        return cache

    def extract_metadata_and_body(self, body):
        """æå–å…ƒæ•°æ®ï¼ˆè¿”å› summary ä¸ºå­—ç¬¦ä¸²ï¼‰"""
        if not body:
            return {"summary": "æš‚æ— ç®€ä»‹", "vertical_title": "", "body": ""}

        lines = body.split('\n')
        summary = "æš‚æ— ç®€ä»‹"
        vertical_title = ""
        meta_indices = []

        for i in range(min(len(lines), 5)):
            line = lines[i].strip()
            if line.startswith('!vml-'):
                match = re.search(r'<span[^>]*>(.*?)</span>', line)
                if match:
                    content = match.group(1).strip()
                    if 'summary' in line:
                        summary = content
                    elif 'title' in line:
                        vertical_title = content
                    meta_indices.append(i)

        content_lines = [l for i, l in enumerate(lines) if i not in meta_indices]
        return {
            "summary": summary,
            "vertical_title": vertical_title,
            "body": "\n".join(content_lines).strip()
        }

    def process_body(self, body):
        """Markdown â†’ HTML"""
        if not body:
            return ""

        html_content = markdown.markdown(
            body,
            extensions=[
                'extra',
                'fenced_code',
                'tables',
                'nl2br',
                'sane_lists',
                'codehilite'
            ],
            extension_configs={
                'codehilite': {
                    'linenums': False,
                    'guess_lang': False,
                    'pygments_style': 'github'
                }
            },
            output_format='html5'
        )

        html_content = re.sub(
            r'<pre><code(?!\s*class=)',
            '<pre><code class="language-plaintext"',
            html_content
        )
        html_content = re.sub(
            r'(<table[^>]*>.*?</table>)',
            r'<div class="table-wrapper">\1</div>',
            html_content,
            flags=re.DOTALL
        )

        return html_content

    def get_issues_articles(self):
        """ä» GitHub Issues è·å–æ–‡ç« æ•°æ®"""
        repo = os.getenv("REPO")
        token = os.getenv("GITHUB_TOKEN")
        if not repo or not token:
            print("âŒ é”™è¯¯: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ REPO (å¦‚ user/repo) å’Œ GITHUB_TOKEN")
            return [], set()

        headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3+json"}

        try:
            url = f"https://api.github.com/repos/{repo}/issues?state=open&per_page=100"
            resp = requests.get(url, headers=headers, timeout=30)
            resp.raise_for_status()
            issues = [i for i in resp.json() if not i.get("pull_request")]
            print(f"âœ… æˆåŠŸè·å– {len(issues)} ç¯‡ GitHub Issues æ–‡ç« ")
            
            # è¿”å› issues åˆ—è¡¨å’Œ ID é›†åˆ
            return issues, {str(i['number']) for i in issues}
        except Exception as e:
            print(f"âŒ GitHub API è¯·æ±‚å¤±è´¥: {e}")
            return [], set()

    def get_local_files_articles(self):
        """ä»æœ¬åœ° docs/posts ç›®å½•è·å–æ–‡ç« æ•°æ®"""
        local_articles = []
        local_ids = set()

        if not os.path.isdir(LOCAL_POSTS_DIR):
            print(f"âš ï¸ æœ¬åœ°æ–‡ç« ç›®å½•ä¸å­˜åœ¨: {LOCAL_POSTS_DIR}")
            return local_articles, local_ids

        md_files = [f for f in os.listdir(LOCAL_POSTS_DIR) if f.lower().endswith('.md')]
        print(f"ğŸ“ åœ¨æœ¬åœ°ç›®å½• {LOCAL_POSTS_DIR} æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
        
        for filename in md_files:
            file_path = os.path.join(LOCAL_POSTS_DIR, filename)
            file_id = os.path.splitext(filename)[0] # å»æ‰ .md åç¼€ä½œä¸º ID
            local_ids.add(file_id)
            
            # --- ä¼˜åŒ–ç‚¹ 3 & 5: è·å–å¹¶ç¼“å­˜ mtime å’Œ iso æ—¶é—´ ---
            try:
                mtime = os.path.getmtime(file_path)
                updated_at_iso = datetime.fromtimestamp(mtime).isoformat()
            except OSError as e:
                print(f"âš ï¸ æ— æ³•è®¿é—®æœ¬åœ°æ–‡ä»¶ {file_path}: {e}, è·³è¿‡")
                continue
            
            # ä¸ºæœ¬åœ°æ–‡ä»¶åˆ›å»ºä¸€ä¸ªç±»ä¼¼ issue çš„ç»“æ„ï¼Œæ–¹ä¾¿åç»­å¤„ç†
            local_article = {
                "id": file_id,
                "title": file_id, # é»˜è®¤æ ‡é¢˜ä¸ºæ–‡ä»¶å
                "created_at": updated_at_iso, # ä½¿ç”¨ä¿®æ”¹æ—¶é—´ä½œä¸ºåˆ›å»ºæ—¶é—´
                "updated_at": updated_at_iso,
                "body": self._read_file_with_fallback(file_path), # è¯»å–æ–‡ä»¶å†…å®¹
                "labels": [] # æœ¬åœ°æ–‡ä»¶é»˜è®¤æ— æ ‡ç­¾
            }
            local_articles.append(local_article)
        
        return local_articles, local_ids

    def _read_file_with_fallback(self, file_path, encodings=['utf-8', 'gbk', 'latin-1']):
        """å°è¯•å¤šç§ç¼–ç è¯»å–æ–‡ä»¶"""
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
        raise ValueError(f"æ— æ³•ä½¿ç”¨å¸¸è§ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")

    def run(self):
        # æ ¹æ®é…ç½®å†³å®šæ•°æ®æº
        all_issues = []
        all_local_articles = []
        remote_ids = set()
        local_ids = set()

        if self.data_source_mode in ['issues_only', 'dual']:
            all_issues, remote_ids = self.get_issues_articles()
        if self.data_source_mode in ['local_only', 'dual']:
            all_local_articles, local_ids = self.get_local_files_articles()
        
        # åˆå¹¶æ‰€æœ‰æ´»è·ƒ ID
        all_active_ids = remote_ids | local_ids

        # === ğŸ”„ æ¸…ç†é€»è¾‘ï¼šç§»é™¤å·²ä¸å­˜åœ¨çš„æºæ‰€å¯¹åº”çš„ç”Ÿæˆç‰© ===
        # è·å–æœ¬åœ°æ‰€æœ‰â€œå·²çŸ¥â€çš„é¡¹ç›® ID é›†åˆ
        known_from_html = {
            f.replace('.html', '') 
            for f in os.listdir(ARTICLE_DIR) 
            if f.endswith('.html')
        }
        known_from_cache = set(self.cache.keys())
        all_known_ids = known_from_html | known_from_cache

        # ç¡®å®šå¾…åˆ é™¤åˆ—è¡¨
        to_delete = all_known_ids - all_active_ids
        
        for item_id in to_delete:
            print(f"ğŸ—‘ï¸ æ¸…ç†å·²ç§»é™¤çš„æ–‡ç« : #{item_id}")
            # --- ä¼˜åŒ–ç‚¹ 5: ç¼“å­˜è·¯å¾„ ---
            html_path = os.path.join(ARTICLE_DIR, f"{item_id}.html")
            # åˆ é™¤ HTML æ–‡ä»¶
            if os.path.exists(html_path):
                os.remove(html_path)
            
            # åˆ é™¤ O-MD ä¸­çš„ Markdown æ–‡ä»¶ (ä»…é€‚ç”¨äºåŸæ¥æºä¸º Issue çš„æ–‡ç« )
            cache_entry = self.cache.get(item_id)
            # ç°åœ¨ cache_entry ä¸€å®šæ˜¯å­—å…¸æ ¼å¼
            if cache_entry and cache_entry.get('type') == 'issue':
                 omd_md_path = os.path.join(OMD_DIR, f"{item_id}.md")
                 if os.path.exists(omd_md_path):
                     os.remove(omd_md_path)
            
            # åˆ é™¤ç¼“å­˜è®°å½•
            if item_id in self.cache:
                del self.cache[item_id]


        # === ğŸ”§ å‡†å¤‡å¤„ç†é€»è¾‘ ===
        to_process_issues = set()
        to_process_local = set()

        # --- å¤„ç† Issues ---
        if self.data_source_mode in ['issues_only', 'dual']:
            for issue in all_issues:
                iid = str(issue['number'])
                updated_at = issue['updated_at']
                
                # --- ä¼˜åŒ–ç‚¹ 4: ä½¿ç”¨é¢„ç”Ÿæˆçš„é›†åˆæ£€æŸ¥ ---
                html_exists = iid in known_from_html
                
                # è·å–ç¼“å­˜é¡¹å¹¶æ£€æŸ¥ç±»å‹å’Œæ—¶é—´
                cached_info = self.cache.get(iid)
                cache_is_issue_type = cached_info and cached_info.get('type') == 'issue'
                cache_time_matches = cached_info and cached_info.get('last_modified') == updated_at

                # ä¹‹å‰ç¼“å­˜äº† issueï¼Œä½† HTML ä¸¢å¤±äº†
                if cache_is_issue_type and cache_time_matches and not html_exists:
                    print(f"âš ï¸ Issue #{iid} HTML ä¸¢å¤±ï¼Œå°†é‡å»º")
                    to_process_issues.add(iid)
                # ä¹‹å‰æ²¡ç¼“å­˜è¿‡
                elif not cached_info:
                    print(f"ğŸ†• æ–° Issue æˆ–ç¼“å­˜ä¸¢å¤±: #{iid}")
                    to_process_issues.add(iid)
                # ç¼“å­˜å­˜åœ¨ä½†æ—¶é—´ä¸åŒ¹é…ï¼ˆå†…å®¹æ›´æ–°ï¼‰
                elif cache_is_issue_type and not cache_time_matches:
                    print(f"ğŸ”„ Issue å†…å®¹å·²æ›´æ–°: #{iid}")
                    to_process_issues.add(iid)

        # --- å¤„ç†æœ¬åœ°æ–‡ä»¶ ---
        if self.data_source_mode in ['local_only', 'dual']:
            for local_article in all_local_articles:
                lid = local_article['id']
                # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»å®æ—¶è·å–mtimeï¼Œå› ä¸ºæ–‡ä»¶å¯èƒ½åœ¨æ­¤æœŸé—´è¢«ä¿®æ”¹
                file_path = os.path.join(LOCAL_POSTS_DIR, f"{lid}.md")
                
                try:
                    current_mtime = os.path.getmtime(file_path)
                    current_mtime_iso = datetime.fromtimestamp(current_mtime).isoformat()
                except OSError:
                    print(f"âš ï¸ æ— æ³•è®¿é—®æœ¬åœ°æ–‡ä»¶ {file_path}, è·³è¿‡: #{lid}")
                    continue
                
                # --- ä¼˜åŒ–ç‚¹ 4: ä½¿ç”¨é¢„ç”Ÿæˆçš„é›†åˆæ£€æŸ¥ ---
                html_exists = lid in known_from_html
                
                # è·å–ç¼“å­˜é¡¹å¹¶æ£€æŸ¥ç±»å‹å’Œæ—¶é—´
                cached_info = self.cache.get(lid)
                cache_is_local_type = cached_info and cached_info.get('type') == 'local_file'
                cache_time_matches = cached_info and cached_info.get('last_modified') == current_mtime_iso

                # ä¹‹å‰ç¼“å­˜äº† local_fileï¼Œä½† HTML ä¸¢å¤±äº†
                if cache_is_local_type and cache_time_matches and not html_exists:
                    print(f"âš ï¸ æœ¬åœ°æ–‡ä»¶ #{lid} HTML ä¸¢å¤±ï¼Œå°†é‡å»º")
                    to_process_local.add(lid)
                # ä¹‹å‰æ²¡ç¼“å­˜è¿‡
                elif not cached_info:
                    print(f"ğŸ†• æ–°æœ¬åœ°æ–‡ä»¶: #{lid}")
                    to_process_local.add(lid)
                # ç¼“å­˜å­˜åœ¨ä½†æ—¶é—´ä¸åŒ¹é…ï¼ˆæ–‡ä»¶æ›´æ–°ï¼‰
                elif cache_is_local_type and not cache_time_matches:
                    print(f"ğŸ”„ æœ¬åœ°æ–‡ä»¶å†…å®¹å·²æ›´æ–°: #{lid}")
                    to_process_local.add(lid)

        # === ğŸ“ å¼€å§‹å¤„ç†éœ€è¦ç”Ÿæˆçš„æ–‡ç«  ===
        all_articles = []
        specials = []
        special_tags = self.config.get('special_tags', [])

        # --- ä¼˜åŒ–ç‚¹ 1: å¼•å…¥ä¸´æ—¶ç¼“å­˜å­—å…¸ç”¨äºæ‰¹é‡æ›´æ–° ---
        new_cache_updates = {}

        # --- å¤„ç† Issues æ–‡ç«  ---
        for issue in all_issues:
            iid = str(issue['number'])
            tags = [label['name'] for label in issue.get('labels', [])]
            is_special = 'special' in tags or 'top' in tags or any(t in tags for t in special_tags)

            # æ„å»ºåˆ—è¡¨é¡¹æ‰€éœ€æ•°æ®ï¼ˆå³ä½¿è·³è¿‡ç”Ÿæˆä¹Ÿè¦æ„å»ºï¼‰
            metadata = self.extract_metadata_and_body(issue.get('body', ''))
            v_title = metadata["vertical_title"] or issue['title'] or "Blog"
            list_item = {
                "id": iid,
                "title": issue['title'],
                "date": issue['created_at'][:10],
                "tags": tags,
                "content": metadata["summary"],
                "url": f"article/{iid}.html",
                "verticalTitle": v_title
            }

            if iid in to_process_issues:
                print(f"ğŸ“ å¤„ç† Issue æ–‡ç« : #{iid} - {issue['title']}")
                processed_html = self.process_body(metadata["body"])

                article_data = {
                    "id": iid,
                    "title": issue['title'],
                    "date": issue['created_at'][:10],
                    "tags": tags,
                    "content": processed_html,
                    "url": f"article/{iid}.html",
                    "verticalTitle": v_title,
                    "summary": metadata["summary"]
                }

                # --- ä¼˜åŒ–ç‚¹ 5: ç¼“å­˜è·¯å¾„ ---
                html_file_path = os.path.join(ARTICLE_DIR, f"{iid}.html")
                # æ¸²æŸ“ HTML
                # --- ä¼˜åŒ–ç‚¹ 2: ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡æ¿ ---
                with open(html_file_path, "w", encoding="utf-8") as f:
                    f.write(self.article_template.render(article=article_data, blog=self.config.get('blog', {})))

                # ä¿å­˜åŸå§‹ Markdown (ä»… Issue)
                # --- ä¼˜åŒ–ç‚¹ 5: ç¼“å­˜è·¯å¾„ ---
                omd_md_file_path = os.path.join(OMD_DIR, f"{iid}.md")
                with open(omd_md_file_path, "w", encoding="utf-8") as f:
                    f.write(issue.get('body') or "")

                # --- ä¼˜åŒ–ç‚¹ 1: å°†ç¼“å­˜æ›´æ–°åŠ å…¥ä¸´æ—¶å­—å…¸ ---
                new_cache_updates[iid] = {
                    "type": "issue",
                    "last_modified": issue['updated_at']
                }
                # self.cache[iid] = { "type": "issue", "last_modified": issue['updated_at'] } # åŸä»£ç 

            # æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨
            if is_special:
                specials.append(list_item)
            else:
                all_articles.append(list_item)

        # --- å¤„ç†æœ¬åœ°æ–‡ä»¶æ–‡ç«  ---
        for local_article in all_local_articles:
            lid = local_article['id']
            # æœ¬åœ°æ–‡ä»¶é»˜è®¤æ— æ ‡ç­¾ï¼Œæ‰€ä»¥ä¸è€ƒè™‘ special
            is_special = False 

            # æ„å»ºåˆ—è¡¨é¡¹æ‰€éœ€æ•°æ®
            metadata = self.extract_metadata_and_body(local_article.get('body', ''))
            v_title = metadata["vertical_title"] or local_article['title'] or "Blog"
            list_item = {
                "id": lid,
                "title": local_article['title'],
                "date": local_article['created_at'][:10],
                "tags": local_article.get('labels', []), # æœ¬åœ°æ–‡ä»¶æ ‡ç­¾ä¸ºç©º
                "content": metadata["summary"],
                "url": f"article/{lid}.html",
                "verticalTitle": v_title
            }

            if lid in to_process_local:
                print(f"ğŸ“ å¤„ç†æœ¬åœ°æ–‡ä»¶æ–‡ç« : #{lid} - {local_article['title']}")
                processed_html = self.process_body(metadata["body"])

                article_data = {
                    "id": lid,
                    "title": local_article['title'],
                    "date": local_article['created_at'][:10],
                    "tags": local_article.get('labels', []),
                    "content": processed_html,
                    "url": f"article/{lid}.html",
                    "verticalTitle": v_title,
                    "summary": metadata["summary"]
                }

                # --- ä¼˜åŒ–ç‚¹ 5: ç¼“å­˜è·¯å¾„ ---
                html_file_path = os.path.join(ARTICLE_DIR, f"{lid}.html")
                # æ¸²æŸ“ HTML
                # --- ä¼˜åŒ–ç‚¹ 2: ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡æ¿ ---
                with open(html_file_path, "w", encoding="utf-8") as f:
                    f.write(self.article_template.render(article=article_data, blog=self.config.get('blog', {})))

                # --- ä¼˜åŒ–ç‚¹ 1: å°†ç¼“å­˜æ›´æ–°åŠ å…¥ä¸´æ—¶å­—å…¸ ---
                # file_path = os.path.join(LOCAL_POSTS_DIR, f"{lid}.md") # è¿™ä¸ªå˜é‡åœ¨å¾ªç¯å¤–å·²å®šä¹‰
                # current_mtime_iso = datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat() # è¿™ä¸ªå˜é‡åœ¨å¾ªç¯å¤–å·²è®¡ç®—
                new_cache_updates[lid] = {
                    "type": "local_file",
                    "last_modified": local_article['updated_at'] # ä½¿ç”¨é¢„è®¡ç®—å¥½çš„ iso æ—¶é—´
                }
                # self.cache[lid] = { "type": "local_file", "last_modified": current_mtime_iso } # åŸä»£ç 

            # æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨ (æœ¬åœ°æ–‡ä»¶ç›®å‰ä¸æ”¯æŒ special æ ‡ç­¾)
            if is_special:
                specials.append(list_item)
            else:
                all_articles.append(list_item)

        # --- ä¼˜åŒ–ç‚¹ 1: ç»Ÿä¸€åº”ç”¨ç¼“å­˜æ›´æ–° ---
        self.cache.update(new_cache_updates)

        # ç‰¹æ®Šå¡ç‰‡ä¿åº•
        if not specials and self.config.get('special', {}).get('view'):
            view = self.config['special']['view']
            run_date_str = view.get('Total_time', '2023.01.01')
            try:
                run_date = datetime.strptime(run_date_str, '%Y.%m.%d')
                days_text = f"è¿è¡Œå¤©æ•°: {(datetime.now() - run_date).days} å¤©"
            except:
                days_text = "è¿è¡Œå¤©æ•°: è®¡ç®—ä¸­..."
            specials.append({
                "id": "0",
                "title": "",
                "date": "",
                "tags": [],
                "content": [
                    view.get('RF_Information', ''),
                    view.get('Copyright', ''),
                    days_text,
                    view.get('Others', '')
                ],
                "url": "",
                "verticalTitle": "Special"
            })
            print("â„¹ï¸ å·²ä»é…ç½®ç”Ÿæˆ Special ä¿¡æ¯")

        all_articles.sort(key=lambda x: x['date'], reverse=True)

        # ä¿å­˜çŠ¶æ€ (æ–°æ ¼å¼)
        with open(OMD_JSON, 'w', encoding='utf-8') as f:
            # --- ä¼˜åŒ–ç‚¹ 1: å†™å…¥çš„æ˜¯å·²åˆå¹¶æ›´æ–°çš„ self.cache ---
            json.dump(self.cache, f, indent=2, ensure_ascii=False)

        base_data = {
            "blog": self.config.get('blog', {}),
            "articles": all_articles,
            "specials": specials,
            "floating_menu": self.config.get('floating_menu', []),
            "special_config": self.config.get('special', {})
        }
        with open(BASE_YAML_OUT, 'w', encoding='utf-8') as f:
            yaml.dump(base_data, f, allow_unicode=True, sort_keys=False)

        # ç”Ÿæˆé¦–é¡µ
        self.generate_index(all_articles, specials)

    def generate_index(self, articles, specials):
        print("ğŸ  æ­£åœ¨ç”Ÿæˆé¦–é¡µ...")
        try:
            # --- ä¼˜åŒ–ç‚¹ 2: ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡æ¿ ---
            # tmpl = self.env.get_template(self.home_template_name) # åŸä»£ç 
            ctx = {
                "BLOG_NAME": self.config.get('blog', {}).get('name', 'VaLog'),
                "SPECIAL_NAME": self.config.get('blog', {}).get('sname', 'Special'),
                "BLOG_DESCRIPTION": self.config.get('blog', {}).get('description', ''),
                "BLOG_AVATAR": self.config.get('blog', {}).get('avatar', ''),
                "BLOG_FAVICON": self.config.get('blog', {}).get('favicon', ''),
                "THEME_MODE": self.config.get('theme', {}).get('mode', 'dark'),
                "PRIMARY_COLOR": self.config.get('theme', {}).get('primary_color', '#e74c3c'),
                "TOTAL_TIME": self.config.get('special', {}).get('view', {}).get('Total_time', '2023.01.01'),
                "ARTICLES_JSON": json.dumps(articles, ensure_ascii=False),
                "SPECIALS_JSON": json.dumps(specials, ensure_ascii=False),
                "MENU_ITEMS_JSON": json.dumps(self.config.get('floating_menu', []), ensure_ascii=False),
                "SPECIAL_TAGS": json.dumps(self.config.get('special_tags', []), ensure_ascii=False),
            }
            with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
                # --- ä¼˜åŒ–ç‚¹ 2: ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡æ¿ ---
                f.write(self.home_template.render(**ctx))
            print("âœ… é¦–é¡µç”Ÿæˆå®Œæ¯•ï¼")
        except Exception as e:
            print(f"âŒ é¦–é¡µç”Ÿæˆé”™è¯¯: {e}")


if __name__ == "__main__":
    try:
        gen = VaLogGenerator()
        gen.run()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        import traceback
        print(f"ğŸ’¥ å‘ç”Ÿæœªé¢„æœŸé”™è¯¯:")
        traceback.print_exc()
        exit(1)